{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1379b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if os.getcwd()[-8:]=='examples':\n",
    "    os.chdir('..')\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" # Possible GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4504274b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "import torchdefenses as td\n",
    "import torchdefenses.trainer as tr\n",
    "from torchdefenses.transforms.cutout import Cutout\n",
    "\n",
    "import src\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45205af5",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7637d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR TRAINING\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#### FOR MODELING\n",
    "MODEL_NAME = \"WRN16-4_WS\"\n",
    "DATA = \"CIFAR10\"\n",
    "MEAN = [0.4930, 0.4875, 0.4548]\n",
    "STD = [0.2475, 0.2445, 0.2637]\n",
    "N_CLASSES = 10\n",
    "LR = 0.1\n",
    "RHO = 0.1\n",
    "\n",
    "#### PATH SAVING\n",
    "PATH = \"./saved/\"\n",
    "NAME = \"cifar-10-warmup\"\n",
    "SAVE_PATH = PATH + NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3fddb",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d2c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR EDM\n",
    "# images = torch.load('./data/cifar-10-edm/cifar10_data_sampled.pt').type('torch.FloatTensor')\n",
    "# labels = torch.load('./data/cifar-10-edm/cifar10_data_sampled_labels.pt').type('torch.LongTensor')\n",
    "\n",
    "# train_transform  = transforms.Compose([\n",
    "#                     transforms.RandomCrop(32, padding=4),\n",
    "#                     transforms.RandomHorizontalFlip(),\n",
    "#                     transforms.Normalize(MEAN, STD),\n",
    "#                     Cutout(),\n",
    "#                    ])\n",
    "\n",
    "## FOR DG\n",
    "data = np.load('./data/cifar-10-edm/cifar10_data_sampled_weight0.npz')\n",
    "images = data['samples'][-50000:]\n",
    "images = [Image.fromarray(np.array(img)) for img in images]\n",
    "labels = torch.Tensor(data['labels'][-50000:]).type('torch.LongTensor')\n",
    "labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "train_transform  = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(MEAN, STD),\n",
    "                    Cutout(),\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9d46f-1cbf-4841-a2d9-05afbabde963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = PtDataset(images, labels, transform=train_transform)\n",
    "public_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                           drop_last=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "data = td.Datasets(data_name=DATA,\n",
    "                   train_transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(MEAN, STD),\n",
    "                    Cutout(),\n",
    "                   ]),\n",
    "                   val_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(MEAN, STD),\n",
    "                   ]),\n",
    "                   test_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(MEAN, STD),\n",
    "                   ]))\n",
    "_, test_loader = data.get_loader(batch_size=BATCH_SIZE, drop_last_train=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44251a2",
   "metadata": {
    "id": "EcestuJGdLLM"
   },
   "source": [
    "### Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c05d5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRN16-4_WS is loaded.\n",
      "model params: 2.7489M\n"
     ]
    }
   ],
   "source": [
    "#### Load model\n",
    "model = src.utils.load_model(model_name=MODEL_NAME, n_classes=N_CLASSES).cuda() # Load model\n",
    "model = ModuleValidator.fix(model)\n",
    "set_groupnorm_num_groups(model, num_groups=16)\n",
    "\n",
    "ModuleValidator.validate(model, strict=False)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"model params: {:.4f}M\".format(pytorch_total_params/1000000))\n",
    "\n",
    "rmodel = td.RobModel(model, n_classes=N_CLASSES, normalization_used={'mean':MEAN, 'std':STD})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b5e92",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f26de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard]\n",
      "Training Information.\n",
      "-Epochs: 100\n",
      "-Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    initial_lr: 0.1\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "-Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fe3b368b820>\n",
      "-Minmizer: <torchdefenses.optim.minimizer.SAM object at 0x7fe39e7555e0>\n",
      "-Save Path: ./saved/cifar-10-warmup\n",
      "-Save Type: None\n",
      "-Record Type: Epoch\n",
      "-Device: cuda:0\n",
      "-----------------------------------------------------------------------\n",
      "Epoch   CALoss   CALoss_1   Clean(Tr)   Clean(Val)   lr       s/it     \n",
      "=======================================================================\n",
      "1       1.1194   1.2725     51.0660     46.4900      0.1000   0.0814   \n",
      "-----------------------------------------------------------------------\n",
      "2       0.9260   1.2273     71.0780     60.6300      0.1000   0.0783   \n",
      "-----------------------------------------------------------------------\n",
      "3       0.4142   0.5942     83.8560     66.0300      0.0999   0.0794   \n",
      "-----------------------------------------------------------------------\n",
      "4       0.1344   0.2807     87.0900     69.0300      0.0998   0.0790   \n",
      "-----------------------------------------------------------------------\n",
      "5       0.3041   0.5362     86.9280     67.8400      0.0996   0.0763   \n",
      "-----------------------------------------------------------------------\n",
      "6       0.4823   0.7292     91.0040     69.1700      0.0994   0.0778   \n",
      "-----------------------------------------------------------------------\n",
      "7       0.1971   0.3857     92.3960     70.5600      0.0991   0.0781   \n",
      "-----------------------------------------------------------------------\n",
      "8       0.3455   0.6289     90.2780     68.9600      0.0988   0.0779   \n",
      "-----------------------------------------------------------------------\n",
      "9       0.3948   0.7159     93.2900     69.3200      0.0984   0.0786   \n",
      "-----------------------------------------------------------------------\n",
      "10      0.0660   0.2496     93.3560     71.7300      0.0980   0.0770   \n",
      "-----------------------------------------------------------------------\n",
      "11      0.5089   0.8737     93.3580     69.9500      0.0976   0.0773   \n",
      "-----------------------------------------------------------------------\n",
      "12      0.0657   0.2296     92.7720     68.9200      0.0970   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "13      0.3715   0.9021     95.8060     72.9000      0.0965   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "14      0.2072   0.4577     95.0840     70.3300      0.0959   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "15      0.4205   0.6912     94.3720     70.9200      0.0952   0.0766   \n",
      "-----------------------------------------------------------------------\n",
      "16      0.2716   0.6835     93.1680     69.7100      0.0946   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "17      0.2339   0.5302     96.4400     75.2500      0.0938   0.0775   \n",
      "-----------------------------------------------------------------------\n",
      "18      0.1251   0.3716     96.4580     74.4900      0.0930   0.0766   \n",
      "-----------------------------------------------------------------------\n",
      "19      0.1347   0.3214     94.8000     70.5300      0.0922   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "20      0.1913   0.4062     96.2660     74.2700      0.0914   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "21      0.1962   0.4568     96.2000     71.9900      0.0905   0.0765   \n",
      "-----------------------------------------------------------------------\n",
      "22      0.1586   0.4639     95.3980     73.0100      0.0895   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "23      0.0807   0.2836     94.5360     71.0300      0.0885   0.0785   \n",
      "-----------------------------------------------------------------------\n",
      "24      0.0772   0.3186     95.6900     72.8000      0.0875   0.0765   \n",
      "-----------------------------------------------------------------------\n",
      "25      0.2665   0.4744     94.3000     71.1400      0.0864   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "26      0.0296   0.2438     95.7760     71.8400      0.0854   0.0776   \n",
      "-----------------------------------------------------------------------\n",
      "27      0.1486   0.4398     96.9340     73.5100      0.0842   0.0766   \n",
      "-----------------------------------------------------------------------\n",
      "28      0.0609   0.1756     96.2400     73.2800      0.0831   0.0770   \n",
      "-----------------------------------------------------------------------\n",
      "29      0.1281   0.3388     96.0160     71.2200      0.0819   0.0762   \n",
      "-----------------------------------------------------------------------\n",
      "30      0.4962   0.9014     97.1620     74.9400      0.0806   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "31      0.0226   0.1430     97.3580     73.8500      0.0794   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "32      0.0897   0.4557     97.2740     74.6800      0.0781   0.0569   \n",
      "-----------------------------------------------------------------------\n",
      "33      0.0576   0.2286     95.6140     72.2000      0.0768   0.0498   \n",
      "-----------------------------------------------------------------------\n",
      "34      0.0573   0.2900     96.5820     72.8200      0.0755   0.0739   \n",
      "-----------------------------------------------------------------------\n",
      "35      0.0318   0.0903     97.1720     73.6100      0.0741   0.0780   \n",
      "-----------------------------------------------------------------------\n",
      "36      0.1283   0.3903     97.8380     75.7800      0.0727   0.0776   \n",
      "-----------------------------------------------------------------------\n",
      "37      0.0357   0.1610     97.6160     76.3500      0.0713   0.0776   \n",
      "-----------------------------------------------------------------------\n",
      "38      0.0719   0.2828     97.8340     74.4800      0.0699   0.0773   \n",
      "-----------------------------------------------------------------------\n",
      "39      0.0521   0.3566     97.6780     74.4100      0.0684   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "40      0.0812   0.3579     97.0120     72.1200      0.0669   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "41      0.0381   0.2888     97.8060     74.8300      0.0655   0.0774   \n",
      "-----------------------------------------------------------------------\n",
      "42      0.0407   0.2202     97.2660     75.1500      0.0639   0.0775   \n",
      "-----------------------------------------------------------------------\n",
      "43      0.0877   0.3219     96.3900     72.1100      0.0624   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "44      0.1111   0.3828     97.5040     74.6200      0.0609   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "45      0.0158   0.1254     97.4520     74.5000      0.0594   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "46      0.0672   0.2956     97.3200     73.6100      0.0578   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "47      0.0578   0.3173     98.1960     75.7800      0.0563   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "48      0.0757   0.2870     98.0900     74.5300      0.0547   0.0779   \n",
      "-----------------------------------------------------------------------\n",
      "49      0.2993   0.6693     98.3860     75.7600      0.0531   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "50      0.0986   0.3698     97.9140     74.5400      0.0516   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "51      0.1632   0.4576     98.6480     76.5300      0.0500   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "52      0.0158   0.2159     98.3600     74.3600      0.0484   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "53      0.0277   0.3931     98.3820     74.4800      0.0469   0.0764   \n",
      "-----------------------------------------------------------------------\n",
      "54      0.0968   0.3496     98.9940     74.9300      0.0453   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "55      0.0766   0.3973     98.7260     75.9500      0.0437   0.0754   \n",
      "-----------------------------------------------------------------------\n",
      "56      0.0322   0.2531     98.6620     76.6700      0.0422   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "57      0.0158   0.1630     99.0340     77.3300      0.0406   0.0776   \n",
      "-----------------------------------------------------------------------\n",
      "58      0.0305   0.2787     99.1860     75.7700      0.0391   0.0751   \n",
      "-----------------------------------------------------------------------\n",
      "59      0.0420   0.3111     98.7560     76.0900      0.0376   0.0785   \n",
      "-----------------------------------------------------------------------\n",
      "60      0.0604   0.3741     99.2020     77.6900      0.0361   0.0785   \n",
      "-----------------------------------------------------------------------\n",
      "61      0.0614   0.3542     99.1920     78.1400      0.0345   0.0774   \n",
      "-----------------------------------------------------------------------\n",
      "62      0.0243   0.2703     98.9480     76.8400      0.0331   0.0775   \n",
      "-----------------------------------------------------------------------\n",
      "63      0.1027   0.3373     99.2180     77.5000      0.0316   0.0776   \n",
      "-----------------------------------------------------------------------\n",
      "64      0.0261   0.2310     99.3640     77.1100      0.0301   0.0786   \n",
      "-----------------------------------------------------------------------\n",
      "65      0.0258   0.2889     99.2420     77.8200      0.0287   0.0778   \n",
      "-----------------------------------------------------------------------\n",
      "66      0.0204   0.1734     99.5180     76.7100      0.0273   0.0778   \n",
      "-----------------------------------------------------------------------\n",
      "67      0.0176   0.3715     99.5720     77.7600      0.0259   0.0783   \n",
      "-----------------------------------------------------------------------\n",
      "68      0.0283   0.1433     99.4640     78.0200      0.0245   0.0775   \n",
      "-----------------------------------------------------------------------\n",
      "69      0.0077   0.1081     99.4940     76.4900      0.0232   0.0756   \n",
      "-----------------------------------------------------------------------\n",
      "70      0.0118   0.1813     99.5800     78.3300      0.0219   0.0785   \n",
      "-----------------------------------------------------------------------\n",
      "71      0.0348   0.2101     99.6300     76.9000      0.0206   0.0779   \n",
      "-----------------------------------------------------------------------\n",
      "72      0.0085   0.1332     99.6500     78.8600      0.0194   0.0771   \n",
      "-----------------------------------------------------------------------\n",
      "73      0.0121   0.2022     99.6620     76.9300      0.0181   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "74      0.0078   0.2828     99.6920     77.2600      0.0169   0.0790   \n",
      "-----------------------------------------------------------------------\n",
      "75      0.0041   0.0483     99.7060     78.9100      0.0158   0.0780   \n",
      "-----------------------------------------------------------------------\n",
      "76      0.0027   0.0960     99.7700     78.9900      0.0146   0.0482   \n",
      "-----------------------------------------------------------------------\n",
      "77      0.0069   0.1189     99.7980     77.8900      0.0136   0.0616   \n",
      "-----------------------------------------------------------------------\n",
      "78      0.0099   0.2643     99.8380     79.5400      0.0125   0.0717   \n",
      "-----------------------------------------------------------------------\n",
      "79      0.0952   0.6351     99.8480     79.9500      0.0115   0.0774   \n",
      "-----------------------------------------------------------------------\n",
      "80      0.0077   0.1589     99.8380     79.1500      0.0105   0.0768   \n",
      "-----------------------------------------------------------------------\n",
      "81      0.0026   0.1670     99.8200     79.1800      0.0095   0.0772   \n",
      "-----------------------------------------------------------------------\n",
      "82      0.0075   0.1324     99.8700     78.5400      0.0086   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "83      0.0050   0.1634     99.8640     79.5200      0.0078   0.0761   \n",
      "-----------------------------------------------------------------------\n",
      "84      0.0027   0.1017     99.9040     80.1600      0.0070   0.0783   \n",
      "-----------------------------------------------------------------------\n",
      "85      0.0037   0.0909     99.9060     79.0900      0.0062   0.0761   \n",
      "-----------------------------------------------------------------------\n",
      "86      0.0119   0.3490     99.9080     79.6900      0.0054   0.0781   \n",
      "-----------------------------------------------------------------------\n",
      "87      0.0114   0.2401     99.9040     79.8500      0.0048   0.0766   \n",
      "-----------------------------------------------------------------------\n",
      "88      0.0028   0.0632     99.9120     80.2400      0.0041   0.0762   \n",
      "-----------------------------------------------------------------------\n",
      "89      0.0030   0.1241     99.9080     80.3300      0.0035   0.0762   \n",
      "-----------------------------------------------------------------------\n",
      "90      0.0038   0.1204     99.9100     79.2100      0.0030   0.0738   \n",
      "-----------------------------------------------------------------------\n",
      "91      0.0118   0.1767     99.9420     79.8500      0.0024   0.0760   \n",
      "-----------------------------------------------------------------------\n",
      "92      0.0065   0.1893     99.9220     80.5700      0.0020   0.0784   \n",
      "-----------------------------------------------------------------------\n",
      "93      0.0015   0.0603     99.9520     80.7200      0.0016   0.0767   \n",
      "-----------------------------------------------------------------------\n",
      "94      0.0017   0.0703     99.9440     80.4100      0.0012   0.0764   \n",
      "-----------------------------------------------------------------------\n",
      "95      0.0038   0.1566     99.9240     80.6800      0.0009   0.0769   \n",
      "-----------------------------------------------------------------------\n",
      "96      0.0016   0.0703     99.9340     80.5500      0.0006   0.0754   \n",
      "-----------------------------------------------------------------------\n",
      "97      0.0012   0.0093     99.9420     80.6800      0.0004   0.0764   \n",
      "-----------------------------------------------------------------------\n",
      "98      0.0021   0.1390     99.9440     80.6800      0.0002   0.0678   \n",
      "-----------------------------------------------------------------------\n",
      "99      0.0051   0.0933     99.9360     80.7700      0.0001   0.0766   \n",
      "-----------------------------------------------------------------------\n",
      "100     0.0032   0.1231     99.9480     80.7600      0.0000   0.0779   \n",
      "-----------------------------------------------------------------------\n",
      "=======================================================================\n",
      "Total Epoch: 100\n",
      "Start Time: 2024-05-30 21:39:17.057619\n",
      "Time Elapsed: 2:13:33.874608\n",
      "Best Records: \n",
      "- Epoch: 99\n",
      "- Iter: 782\n",
      "- Clean(Val): 80.7700\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer = tr.Standard(rmodel)\n",
    "trainer.setup(optimizer=f\"SGD(lr={LR}, momentum=0.9, weight_decay=0.0005)\",\n",
    "              scheduler=\"Cosine\", scheduler_type=\"Epoch\",\n",
    "              minimizer=f\"SAM(rho={RHO})\",\n",
    "              n_epochs=EPOCHS, n_iters=len(public_loader)\n",
    "             )\n",
    "trainer.record_rob(public_loader, test_loader, eps=None)\n",
    "trainer.fit(train_loader=public_loader, n_epochs=EPOCHS,\n",
    "            save_path=SAVE_PATH, save_best={\"Clean(Val)\":\"HBO\"},\n",
    "            save_type=None, save_overwrite=True, record_type=\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1db40",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0145f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Record Info:\n",
      "OrderedDict([('Epoch', 100), ('Iter', 782), ('CALoss', 0.003191609401255846), ('CALoss_1', 0.12312682718038559), ('Clean(Tr)', 99.948), ('Clean(Val)', 80.76), ('lr', 2.4671981713420017e-05), ('s/it', 0.0778921189258312)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmodel.load_dict(PATH+NAME+'/last.pth')\n",
    "rmodel.eval_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5737d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Record Info:\n",
      "OrderedDict([('Epoch', 99), ('Iter', 782), ('CALoss', 0.005149493459612131), ('CALoss_1', 0.09331594407558441), ('Clean(Tr)', 99.936), ('Clean(Val)', 80.77), ('lr', 9.866357858642213e-05), ('s/it', 0.07660343606138108)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmodel.load_dict(PATH+NAME+'/best.pth')\n",
    "rmodel.eval_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8209fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d83b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc78a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
